{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-11T04:44:29.293114Z","iopub.execute_input":"2023-09-11T04:44:29.293550Z","iopub.status.idle":"2023-09-11T04:44:32.476628Z","shell.execute_reply.started":"2023-09-11T04:44:29.293515Z","shell.execute_reply":"2023-09-11T04:44:32.475524Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"training_data = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntraining_data = np.array(training_data)\nm, n = training_data.shape\nnp.random.shuffle(training_data) # shuffle before splitting into dev and training sets\n\ndata_train = training_data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n] / 255.\n_,m_train = X_train.shape\n\nY_train   # array of correct values for each observation","metadata":{"execution":{"iopub.status.busy":"2023-09-11T04:45:17.286431Z","iopub.execute_input":"2023-09-11T04:45:17.286877Z","iopub.status.idle":"2023-09-11T04:45:21.330583Z","shell.execute_reply.started":"2023-09-11T04:45:17.286842Z","shell.execute_reply":"2023-09-11T04:45:21.329393Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"array([3, 0, 0, ..., 2, 4, 7])"},"metadata":{}}]},{"cell_type":"code","source":"# initialize parameters\ndef init():\n    W1 = np.random.rand(10, 784) - 0.5   # - 0.5 to adjust values to be between -0.5 and +0.5\n    b1 = np.random.rand(10, 1) - 0.5\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2\n\n# ReLU function returns 0 if the given value x is negative and x if the given value x is positive \ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\n# takes in value from output layer and returns a probability that the value is correct\ndef softmax(Z):\n    A = np.exp(Z) / sum(np.exp(Z))\n    return A\n\n# step 1\ndef f_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = softmax(Z2)\n    return Z1, A1, Z2, A2\n\n# since ReLU function return 0 if x is negative and x otherwise,\n# its derivative shows that slope is 1 if x is positive and 0 otherwise\n# therefore return true (1) if x is positive, and false (0) otherwise\ndef d_ReLU(Z):\n    return Z > 0\n\n# returns a column where all entries are 0 except for the index matching the correct value of the observation\ndef mat_one(Y):\n    one_Y = np.zeros((Y.size, Y.max() + 1))\n    one_Y[np.arange(Y.size), Y] = 1\n    one_Y = one_Y.T  # change from row to column to match our matrix\n    return one_Y\n\n# undoes f_prop\ndef b_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_Y = mat_one(Y)\n    dZ2 = A2 - one_Y\n    dW2 = 1 / m * dZ2.dot(A1.T)\n    db2 = 1 / m * np.sum(dZ2, axis=1).reshape(-1, 1)\n    dZ1 = W2.T.dot(dZ2) * d_ReLU(Z1)\n    dW1 = 1 / m * dZ1.dot(X.T)\n    db1 = 1 / m * np.sum(dZ1, axis=1).reshape(-1, 1)\n    return dW1, db1, dW2, db2\n\ndef update(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2\n\ndef predict(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    #print(predictions, Y)\n    return np.sum(predictions == Y) / Y.size\n\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = f_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = b_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = predict(A2)\n            print(\"Accuracy: \", get_accuracy(predictions, Y))\n    return W1, b1, W2, b2","metadata":{"execution":{"iopub.status.busy":"2023-09-11T04:51:35.258076Z","iopub.execute_input":"2023-09-11T04:51:35.258522Z","iopub.status.idle":"2023-09-11T04:51:35.283819Z","shell.execute_reply.started":"2023-09-11T04:51:35.258486Z","shell.execute_reply":"2023-09-11T04:51:35.282834Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# accuracy on training set is 85.5%. can definitely improve by adding more layers \n# (right now we're only calculating two layers)\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.1, 500)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T04:55:36.707341Z","iopub.execute_input":"2023-09-11T04:55:36.707792Z","iopub.status.idle":"2023-09-11T04:56:53.345382Z","shell.execute_reply.started":"2023-09-11T04:55:36.707755Z","shell.execute_reply":"2023-09-11T04:56:53.343737Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Iteration:  0\n[8 3 2 ... 7 0 0] [3 0 0 ... 2 4 7]\nAccuracy:  0.111\nIteration:  10\n[8 3 2 ... 1 0 0] [3 0 0 ... 2 4 7]\nAccuracy:  0.1985609756097561\nIteration:  20\n[8 3 2 ... 1 9 0] [3 0 0 ... 2 4 7]\nAccuracy:  0.28424390243902437\nIteration:  30\n[8 0 2 ... 2 9 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.35397560975609754\nIteration:  40\n[5 0 6 ... 2 9 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.4222439024390244\nIteration:  50\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.480390243902439\nIteration:  60\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.5299512195121951\nIteration:  70\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.5714146341463414\nIteration:  80\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.6072439024390244\nIteration:  90\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.6390487804878049\nIteration:  100\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.6648780487804878\nIteration:  110\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.6860487804878049\nIteration:  120\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7050487804878048\nIteration:  130\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7195853658536585\nIteration:  140\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7327317073170732\nIteration:  150\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7440487804878049\nIteration:  160\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7533414634146341\nIteration:  170\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7616829268292683\nIteration:  180\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7691951219512195\nIteration:  190\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7760975609756098\nIteration:  200\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7822926829268293\nIteration:  210\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7883658536585366\nIteration:  220\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7942926829268293\nIteration:  230\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.7987560975609757\nIteration:  240\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8034878048780488\nIteration:  250\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8076585365853659\nIteration:  260\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8112439024390244\nIteration:  270\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8147073170731707\nIteration:  280\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8179512195121951\nIteration:  290\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8205853658536585\nIteration:  300\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8232195121951219\nIteration:  310\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.826\nIteration:  320\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8285365853658536\nIteration:  330\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8308048780487804\nIteration:  340\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8328780487804878\nIteration:  350\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8349756097560975\nIteration:  360\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8367560975609756\nIteration:  370\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8386341463414634\nIteration:  380\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8402439024390244\nIteration:  390\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8419512195121951\nIteration:  400\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.843170731707317\nIteration:  410\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8445121951219512\nIteration:  420\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8456585365853658\nIteration:  430\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8471951219512195\nIteration:  440\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8484634146341463\nIteration:  450\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8499512195121951\nIteration:  460\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8512439024390244\nIteration:  470\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8526585365853658\nIteration:  480\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8539756097560975\nIteration:  490\n[5 0 0 ... 2 4 7] [3 0 0 ... 2 4 7]\nAccuracy:  0.8548536585365853\n","output_type":"stream"}]}]}